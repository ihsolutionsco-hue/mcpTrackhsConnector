# Testing Guide - TrackHS MCP Connector

Esta guÃ­a explica cÃ³mo ejecutar y agregar tests para el TrackHS MCP Connector.

## ðŸ“‹ Ãndice

- [ConfiguraciÃ³n de Testing](#configuraciÃ³n-de-testing)
- [Ejecutar Tests](#ejecutar-tests)
- [Estructura de Tests](#estructura-de-tests)
- [Agregar Nuevos Tests](#agregar-nuevos-tests)
- [Cobertura de CÃ³digo](#cobertura-de-cÃ³digo)
- [Testing Local](#testing-local)
- [CI/CD Testing](#cicd-testing)

## ðŸ”§ ConfiguraciÃ³n de Testing

### Prerrequisitos

```bash
# Instalar dependencias de testing
pip install -r requirements-dev.txt

# Configurar variables de entorno
cp env.example .env
# Editar .env con tus credenciales
```

### Estructura de Testing

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ unit/                    # Tests unitarios
â”‚   â”œâ”€â”€ test_api_client.py
â”‚   â”œâ”€â”€ test_auth.py
â”‚   â”œâ”€â”€ test_types.py
â”‚   â”œâ”€â”€ test_error_handling.py
â”‚   â”œâ”€â”€ test_pagination.py
â”‚   â”œâ”€â”€ test_logging.py
â”‚   â””â”€â”€ test_completion.py
â”œâ”€â”€ integration/            # Tests de integraciÃ³n
â”‚   â”œâ”€â”€ test_search_reservations.py
â”‚   â”œâ”€â”€ test_resources.py
â”‚   â””â”€â”€ test_prompts.py
â””â”€â”€ e2e/                   # Tests end-to-end
    â”œâ”€â”€ test_server.py
    â””â”€â”€ test_mcp_integration.py
```

## ðŸš€ Ejecutar Tests

### Tests Unitarios

```bash
# Ejecutar todos los tests unitarios
pytest tests/unit/ -v

# Ejecutar test especÃ­fico
pytest tests/unit/test_api_client.py -v

# Ejecutar con cobertura
pytest tests/unit/ -v --cov=src --cov-report=html
```

### Tests de IntegraciÃ³n

```bash
# Ejecutar todos los tests de integraciÃ³n
pytest tests/integration/ -v

# Ejecutar test especÃ­fico
pytest tests/integration/test_search_reservations.py -v
```

### Tests E2E

```bash
# Ejecutar todos los tests E2E
pytest tests/e2e/ -v

# Ejecutar test especÃ­fico
pytest tests/e2e/test_server.py -v
```

### Todos los Tests

```bash
# Ejecutar todos los tests
pytest tests/ -v

# Ejecutar con marcadores especÃ­ficos
pytest -m "unit" -v
pytest -m "integration" -v
pytest -m "e2e" -v
```

## ðŸ“Š Cobertura de CÃ³digo

### Generar Reporte de Cobertura

```bash
# Generar reporte HTML
pytest tests/ --cov=src --cov-report=html

# Generar reporte XML
pytest tests/ --cov=src --cov-report=xml

# Ver cobertura en terminal
pytest tests/ --cov=src --cov-report=term-missing
```

### Verificar Cobertura MÃ­nima

```bash
# Requerir 80% de cobertura
pytest tests/ --cov=src --cov-fail-under=80
```

## ðŸ§ª Testing Local

### Script de Testing Local

```bash
# Ejecutar script de testing local
python test_local.py
```

Este script verifica:
- âœ… ConfiguraciÃ³n de variables de entorno
- âœ… InicializaciÃ³n del API client
- âœ… Registro de herramientas MCP
- âœ… Registro de recursos MCP
- âœ… Registro de prompts MCP
- âœ… Sistema de manejo de errores

### Testing Manual

```bash
# Ejecutar servidor en modo desarrollo
python src/trackhs_mcp/server.py

# En otra terminal, probar conexiÃ³n
curl http://localhost:8000/health
```

## ðŸ”„ CI/CD Testing

### GitHub Actions

Los tests se ejecutan automÃ¡ticamente en:
- **Push a main**: Tests completos + deployment
- **Pull Request**: Tests completos + linting
- **Manual**: Tests completos + security scan

### ConfiguraciÃ³n de Secrets

```bash
# Configurar secrets en GitHub
TRACKHS_API_URL=https://api.trackhs.com/api
TRACKHS_USERNAME=your_username
TRACKHS_PASSWORD=your_password
TRACKHS_TIMEOUT=30
FASTMCP_API_KEY=your_api_key
```

## ðŸ“ Agregar Nuevos Tests

### Test Unitario

```python
# tests/unit/test_new_feature.py
import pytest
from src.trackhs_mcp.new_feature import NewFeature

class TestNewFeature:
    @pytest.mark.unit
    def test_new_feature_basic(self):
        feature = NewFeature()
        result = feature.process("input")
        assert result == "expected_output"
    
    @pytest.mark.unit
    @pytest.mark.asyncio
    async def test_new_feature_async(self):
        feature = NewFeature()
        result = await feature.process_async("input")
        assert result == "expected_output"
```

### Test de IntegraciÃ³n

```python
# tests/integration/test_new_integration.py
import pytest
from unittest.mock import Mock, AsyncMock

class TestNewIntegration:
    @pytest.fixture
    def mock_api_client(self):
        client = Mock()
        client.get = AsyncMock()
        return client
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_new_integration(self, mock_api_client):
        mock_api_client.get.return_value = {"data": "test"}
        
        # Test integration logic
        result = await some_integration_function(mock_api_client)
        assert result["data"] == "test"
```

### Test E2E

```python
# tests/e2e/test_new_e2e.py
import pytest
from unittest.mock import Mock, AsyncMock

class TestNewE2E:
    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_complete_workflow(self):
        # Test complete end-to-end workflow
        result = await complete_workflow()
        assert result["status"] == "success"
```

## ðŸ·ï¸ Marcadores de Tests

### Marcadores Disponibles

```python
@pytest.mark.unit          # Tests unitarios
@pytest.mark.integration   # Tests de integraciÃ³n
@pytest.mark.e2e          # Tests end-to-end
@pytest.mark.slow         # Tests lentos
@pytest.mark.api          # Tests que requieren API
@pytest.mark.auth         # Tests de autenticaciÃ³n
@pytest.mark.network      # Tests que requieren red
```

### Ejecutar por Marcador

```bash
# Solo tests unitarios
pytest -m "unit" -v

# Solo tests de integraciÃ³n
pytest -m "integration" -v

# Excluir tests lentos
pytest -m "not slow" -v

# Solo tests que requieren API
pytest -m "api" -v
```

## ðŸ” Debugging Tests

### Ejecutar con Debug

```bash
# Ejecutar con output detallado
pytest tests/ -v -s

# Ejecutar con pdb
pytest tests/ --pdb

# Ejecutar test especÃ­fico con debug
pytest tests/unit/test_api_client.py::TestTrackHSApiClient::test_request_success -v -s --pdb
```

### Logging en Tests

```python
import logging

def test_with_logging():
    logging.basicConfig(level=logging.DEBUG)
    logger = logging.getLogger(__name__)
    
    logger.debug("Test starting")
    # Test logic
    logger.debug("Test completed")
```

## ðŸ“ˆ Performance Testing

### Tests de Rendimiento

```bash
# Ejecutar tests de rendimiento
pytest tests/ -k "performance" --benchmark-only

# Generar reporte de rendimiento
pytest tests/ --benchmark-only --benchmark-save=performance-report
```

### Memory Testing

```bash
# Ejecutar tests de memoria
pytest tests/e2e/test_mcp_integration.py::TestMCPIntegrationE2E::test_memory_usage_workflow -v
```

## ðŸ›¡ï¸ Security Testing

### Tests de Seguridad

```bash
# Ejecutar bandit (security linter)
bandit -r src/

# Ejecutar safety (vulnerability check)
safety check

# Ejecutar semgrep (security scanner)
semgrep --config=auto src/
```

## ðŸ“‹ Checklist de Testing

### Antes de Commit

- [ ] Tests unitarios pasan
- [ ] Tests de integraciÃ³n pasan
- [ ] Tests E2E pasan
- [ ] Cobertura >= 80%
- [ ] Linting pasa
- [ ] Security scan pasa

### Antes de Deploy

- [ ] Todos los tests pasan
- [ ] Performance tests pasan
- [ ] Security tests pasan
- [ ] Documentation actualizada
- [ ] Secrets configurados

## ðŸš¨ Troubleshooting

### Problemas Comunes

1. **ImportError**: Verificar que `src/` estÃ¡ en el PYTHONPATH
2. **AsyncIO errors**: Usar `pytest-asyncio` y `@pytest.mark.asyncio`
3. **Mock errors**: Verificar que los mocks estÃ¡n configurados correctamente
4. **Timeout errors**: Aumentar timeout en configuraciÃ³n

### Soluciones

```bash
# Verificar imports
python -c "from src.trackhs_mcp.server import main"

# Verificar async
pytest tests/ --asyncio-mode=auto

# Verificar mocks
pytest tests/ -v -s --tb=short
```

## ðŸ“š Recursos Adicionales

- [pytest Documentation](https://docs.pytest.org/)
- [pytest-asyncio Documentation](https://pytest-asyncio.readthedocs.io/)
- [FastMCP Testing Guide](https://fastmcp.dev/testing)
- [Python Testing Best Practices](https://docs.python.org/3/library/unittest.html)
