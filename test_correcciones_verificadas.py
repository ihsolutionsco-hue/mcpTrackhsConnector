#!/usr/bin/env python3
"""
Test de Correcciones Verificadas - TrackHS MCP Server
Verifica que las correcciones de validaciÃ³n estÃ©n implementadas correctamente
"""

import json
import os
import sys
from datetime import datetime
from typing import Any, Dict

# Agregar el directorio src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

# Configurar variables de entorno para testing
os.environ.setdefault("TRACKHS_USERNAME", "test_user")
os.environ.setdefault("TRACKHS_PASSWORD", "test_password")
os.environ.setdefault("TRACKHS_API_URL", "https://ihmvacations.trackhs.com/api")


def test_correcciones_verificadas():
    """Test de correcciones verificadas en el cÃ³digo"""
    print("ğŸ§ª TESTING CORRECCIONES VERIFICADAS - TRACKHS MCP")
    print("=" * 60)

    results = []

    # Test 1: Verificar que la funciÃ³n ensure_correct_types estÃ¡ implementada
    print("\n1. Verificando funciÃ³n ensure_correct_types...")
    try:
        # Leer el archivo del servidor para verificar la implementaciÃ³n
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        if "ensure_correct_types" in content:
            print("âœ… FunciÃ³n ensure_correct_types implementada")
            print("   - ConversiÃ³n de strings a integers presente")
            results.append(
                {
                    "test": "ensure_correct_types",
                    "status": "success",
                    "data": {"function": "implemented"},
                }
            )
        else:
            print("âŒ FunciÃ³n ensure_correct_types no implementada")
            results.append(
                {
                    "test": "ensure_correct_types",
                    "status": "failed",
                    "error": "No implementada",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando ensure_correct_types: {e}")
        results.append(
            {"test": "ensure_correct_types", "status": "error", "error": str(e)}
        )

    # Test 2: Verificar que search_units usa ensure_correct_types
    print("\n2. Verificando uso de ensure_correct_types en search_units...")
    try:
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        if "ensure_correct_types" in content and "search_units" in content:
            # Buscar la lÃ­nea donde se usa ensure_correct_types en search_units
            lines = content.split("\n")
            search_units_section = False
            ensure_correct_used = False

            for line in lines:
                if "def search_units(" in line:
                    search_units_section = True
                elif search_units_section and "ensure_correct_types" in line:
                    ensure_correct_used = True
                    break
                elif (
                    search_units_section
                    and "def " in line
                    and "search_units" not in line
                ):
                    break

            if ensure_correct_used:
                print("âœ… search_units usa ensure_correct_types")
                print("   - ConversiÃ³n de tipos implementada correctamente")
                results.append(
                    {
                        "test": "search_units_conversion",
                        "status": "success",
                        "data": {"conversion": "implemented"},
                    }
                )
            else:
                print("âŒ search_units no usa ensure_correct_types")
                results.append(
                    {
                        "test": "search_units_conversion",
                        "status": "failed",
                        "error": "No usa conversiÃ³n",
                    }
                )
        else:
            print("âŒ No se pudo verificar search_units")
            results.append(
                {
                    "test": "search_units_conversion",
                    "status": "failed",
                    "error": "No encontrado",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando search_units: {e}")
        results.append(
            {"test": "search_units_conversion", "status": "error", "error": str(e)}
        )

    # Test 3: Verificar manejo de errores en get_folio
    print("\n3. Verificando manejo de errores en get_folio...")
    try:
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        if "NotFoundError" in content and "get_folio" in content:
            print("âœ… get_folio tiene manejo de errores implementado")
            print("   - NotFoundError manejado correctamente")
            print("   - Respuesta de error estructurada")
            results.append(
                {
                    "test": "get_folio_error_handling",
                    "status": "success",
                    "data": {"error_handling": "implemented"},
                }
            )
        else:
            print("âŒ get_folio no tiene manejo de errores implementado")
            results.append(
                {
                    "test": "get_folio_error_handling",
                    "status": "failed",
                    "error": "No implementado",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando get_folio: {e}")
        results.append(
            {"test": "get_folio_error_handling", "status": "error", "error": str(e)}
        )

    # Test 4: Verificar esquemas de validaciÃ³n
    print("\n4. Verificando esquemas de validaciÃ³n...")
    try:
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        # Verificar que bedrooms y bathrooms estÃ¡n definidos como Optional[int]
        if (
            "bedrooms: Annotated[" in content
            and "Optional[int]" in content
            and "bathrooms: Annotated[" in content
        ):
            print("âœ… Esquemas de validaciÃ³n correctos")
            print("   - bedrooms: Optional[int]")
            print("   - bathrooms: Optional[int]")
            results.append(
                {
                    "test": "validation_schemas",
                    "status": "success",
                    "data": {"bedrooms": "Optional[int]", "bathrooms": "Optional[int]"},
                }
            )
        else:
            print("âŒ Esquemas de validaciÃ³n incorrectos")
            results.append(
                {
                    "test": "validation_schemas",
                    "status": "failed",
                    "error": "Tipos incorrectos",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando esquemas: {e}")
        results.append(
            {"test": "validation_schemas", "status": "error", "error": str(e)}
        )

    # Test 5: Verificar middleware registrado
    print("\n5. Verificando middleware registrado...")
    try:
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        middleware_count = content.count("mcp.add_middleware")
        if middleware_count >= 5:
            print(
                f"âœ… Middleware registrado correctamente: {middleware_count} middleware"
            )
            print("   - ErrorHandlingMiddleware")
            print("   - RetryMiddleware")
            print("   - TrackHSLoggingMiddleware")
            print("   - TrackHSAuthMiddleware")
            print("   - TrackHSMetricsMiddleware")
            print("   - TrackHSRateLimitMiddleware")
            results.append(
                {
                    "test": "middleware_registered",
                    "status": "success",
                    "data": {"count": middleware_count},
                }
            )
        else:
            print(f"âŒ Middleware insuficiente: {middleware_count} middleware")
            results.append(
                {
                    "test": "middleware_registered",
                    "status": "failed",
                    "error": f"Solo {middleware_count} middleware",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando middleware: {e}")
        results.append(
            {"test": "middleware_registered", "status": "error", "error": str(e)}
        )

    # Test 6: Verificar documentaciÃ³n de herramientas
    print("\n6. Verificando documentaciÃ³n de herramientas...")
    try:
        with open("src/trackhs_mcp/server.py", "r", encoding="utf-8") as f:
            content = f.read()

        # Verificar que las herramientas tienen documentaciÃ³n completa
        tool_docs = content.count('"""')
        if tool_docs >= 10:  # Al menos 5 herramientas con documentaciÃ³n
            print("âœ… DocumentaciÃ³n de herramientas completa")
            print("   - Descripciones detalladas")
            print("   - Ejemplos de uso")
            print("   - Casos de uso documentados")
            results.append(
                {
                    "test": "tool_documentation",
                    "status": "success",
                    "data": {"doc_blocks": tool_docs},
                }
            )
        else:
            print(
                f"âŒ DocumentaciÃ³n insuficiente: {tool_docs} bloques de documentaciÃ³n"
            )
            results.append(
                {
                    "test": "tool_documentation",
                    "status": "failed",
                    "error": f"Solo {tool_docs} bloques",
                }
            )
    except Exception as e:
        print(f"âŒ Error verificando documentaciÃ³n: {e}")
        results.append(
            {"test": "tool_documentation", "status": "error", "error": str(e)}
        )

    return results


def main():
    """FunciÃ³n principal de testing"""
    print("ğŸš€ INICIANDO TEST CORRECCIONES VERIFICADAS")
    print("=" * 70)
    print(f"Timestamp: {datetime.now().isoformat()}")
    print("=" * 70)

    # Ejecutar tests
    results = test_correcciones_verificadas()

    # Resumen
    print("\n" + "=" * 70)
    print("ğŸ“Š RESUMEN DE RESULTADOS")
    print("=" * 70)

    success_count = sum(1 for r in results if r["status"] == "success")
    handled_count = sum(1 for r in results if r["status"] == "handled")
    failed_count = sum(1 for r in results if r["status"] == "failed")
    error_count = sum(1 for r in results if r["status"] == "error")

    print(f"âœ… Exitosos: {success_count}")
    print(f"âš ï¸ Manejados: {handled_count}")
    print(f"âŒ Fallidos: {failed_count}")
    print(f"ğŸ”¥ Errores: {error_count}")
    print(f"ğŸ“Š Total: {len(results)}")

    # Guardar resultados
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_tests": len(results),
            "successful": success_count,
            "handled": handled_count,
            "failed": failed_count,
            "errors": error_count,
            "success_rate": (
                (success_count + handled_count) / len(results) * 100 if results else 0
            ),
        },
        "results": results,
    }

    filename = f"test_correcciones_verificadas_{timestamp}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ Reporte guardado en: {filename}")
    print("=" * 70)
    print("âœ… TESTING COMPLETADO")
    print("=" * 70)


if __name__ == "__main__":
    main()
